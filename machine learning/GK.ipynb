{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ôn tập kiến thức thi giữa kì\n",
    "- Chương 1: Giới thiệu máy học.\n",
    "- Chương 2: Mô hình cây quyết định (Decision Tree).\n",
    "- Chương 3: Mô hình mạng nơ ron (Neural Network).\n",
    "- Chương 4: Mô hình theo nhớm kết hợp (Boosting/ Adaboost).\n",
    "- Chương 5: Mô hình thuật giải di truyền đối với bài toán phân lớp (Genetic Algorithm)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương 1: Giới thiệu máy học\n",
    "- Máy học là để chỉ đến việc xác định, xây dựng và đánh giá mô hình tính toán dùng để tái tạo, dự đoán hay đưa ra quyết định từ tâp dữ liệu cho trước."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương 2: Cây quyết định (Devision Tree)\n",
    "- Các vấn đề trong việc xây dựng và áp dụng cây quyết định:\n",
    "  + Lựa chọn độ đo và xây dựng các node trong cây.\n",
    "  + Xử lý data bị mất hoặc bị thiếu trong bảng dữ liệu.\n",
    "  + Xử lý các trường hợp quá khớp (overfitting).\n",
    "  - Xử lý rút gọn cây qua quá trình sắp xếp lại và loại bỏ nhánh để tránh cây trùng.\n",
    "\n",
    "# Vocaburary\n",
    "- Covariance: hiệp phương sai: là thước đo khoảng cách chênh lệch giữa các số liệu trong tập dữ liệu với nhau so với giá trị trung bình của tập dữ liệu, phương sai là bình phương của độ lệch chuẩn: Công thưc tính = tổng bình phương độ lệch:\n",
    "  + Phương sai đo được khả năng rủi ro.\n",
    "- Hiệp phương sai: cho biết hai goạc nhiều biến có biến thiên cùng chiều hay không: hai chiều biến thiên cùng chiều sẽ có giá trị hiệp phương sai dương\n",
    "- means(giá trị trung bình)\n",
    "\n",
    "# Slide:\n",
    "- Nút gốc, nút nội bộ: tên các thuộc tính của tập dữ liệu.\n",
    "- Nút lá: tên các cj\n",
    "- Nhánh: tương ứng với các giá trị của các thuộc tính.\n",
    "\n",
    "=> Phân lớp các mẫu bằng cách duyệt cây từ gốc tới lá\n",
    "- Ý tưởng chính: Xây dưng cây top down để cho ra kết quả tốt nhất\n",
    "  + Thuộc tính cho ta cây đơn giản nhất (Định luật Occam)\n",
    "  + Heuristic: thuộc tính tạo ra nút có tính đồng nhất cao\n",
    "  + Sự dụng các độ đo đồng nhất: Information Gain (Entropy), Information Gain Ratio, Gini Index.\n",
    "- Điều kiện dừng:\n",
    "  + Không còn thuộc tính nào được phân lớp nữa\n",
    "  + Tất cả các mẫu đều đã được phân lớp\n",
    "\n",
    "# Độ lợi thông tin - Information Gain (Chọn thuộc tính lớn nhất)\n",
    "- Gain: độ lợi thông tin\n",
    "- Thông tin kì vọng (entropy: sự hỗn loạn): để phân lớp một mẫu trong D là:\n",
    "- Gain(A) = Info(D) - InfoA (D): Thuộc tính nào có gain lớn nhất sẽ được chọn làm nút gốc\n",
    "\n",
    "# Information Gain Ratio - Độ lợi thông tin theo tỉ lệ\n",
    "- Độ đo Gain có xun hướng thiên vị cho các thuộc tính có nhiều giá trị (tạo ra nhiều nhánh).\n",
    "- Độ đo Gain Ratio quan tâm đên số lượng và độ lớn của các nhánh khi lựa chọn thuộc tính phân lớp\n",
    "- GainRatio(A) = Gain(A)/ SplitInfoA(D)\n",
    "\n",
    "# Chỉ mục Gini - Gini Index (Chọn thuộc tính nhỏ nhất)\n",
    "- Chọn thuộc tính có Gini nhỏ nhất để phân chia tập dữ liệu\n",
    "\n",
    "=> Nhận xét 3 độ đo:\n",
    "- Information Gain: Có xu hướng thiên vị cho các thuộc tính có nhiều giá trị\n",
    "- Information Gain Ratio: CÓ xu hướng chọn thuộc tính phân lớp tạo ra một cây con nhỏ hơn nhiều so với các cây con khác.\n",
    "- Gini Index: \n",
    "  + Có xu hướng thiên vị cho các thuộc tính có phân hoạch thành các tập lớn\n",
    "  + Gặp khó khăn khi số lượng lớp lớn\n",
    "  + Có xu hướng chọn thuộc tính phân lớp tạo ra các cây con kích thước và độ hỗn loạn bằng nhau.\n",
    "\n",
    "# Perceptron learning algorithm (PLA):\n",
    "- Ý tưởng quan trọng của thật toán: cứ là đi sai đâu sửa đấy.\n",
    "- Là một dạng bài toán classification.\n",
    "- Tìm biên giới (boundary)\n",
    "- Nếu tồn tại một mặt phằng chia hai lớp thì ta gọi lớp đó là linearly separate (có thể tách rời tuyến tính), đường thẳng đó gọi là Linear Classifier:\n",
    "- Các điểm bị phân lớp lỗi gọi là: misclassified\n",
    "- Gradient decent và Stochatic gradient decent (để tối ưu hàm mất mát)\n",
    "\n",
    "# Rút gọn cây:\n",
    "- Rút gọn cây được thực hiện như thế nào\n",
    "- Tạo sao lại phải rút gọn cây\n",
    "- Rút gọn cây (tỉa cây):\n",
    "  + Xác định và loại bỏ các nhánh không ổn định hoặc cá biệt\n",
    "  + Cây được rút gọn có xu hướng nhỏ lại và ít phức tạp hơn nên dễ hiểu hơn\n",
    "  + Cây được rút gọn này phân lớp nhanh hơn và tốt hơn\n",
    "- 3 hướng tiếp cần thông thường để rút gọn cây:\n",
    "  + Trước Pre-pruning\n",
    "  + Sau Post-pruning: Ngoài ra co thể áp dụng nguyên lý MDL (Minimum Description Length): tức là rút gọn cây dựa trên số lượng bit dùng để mã hóa thay vì đo lường tần suất lỗi.\n",
    "  - Kết hợp\n",
    "\n",
    "# Note:\n",
    "- Đọc lại phần Gain Ratio tính SplitInfoX\n",
    "- Hiểu hơn về phương pháp rút gọn cây và bài toán thực tiễn\n",
    "- Đọc lại 3 loại phân phối"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương 3: Mô hình mạng nơ ron (neural network)\n",
    "- Mô hình perceptron.\n",
    "- Mô hình mạng đa lớp.\n",
    "- Mô hình mạng 2 lớp.\n",
    "- Nghiên cứu thuật toán học và đánh giá các mô hình trên.\n",
    "\n",
    "# Slide:\n",
    "- Hệ thống ANN tiêu biểu\n",
    "\n",
    "# Khái niệm perceptron\n",
    "- Nhận tín hiệu từ dữ liệu nhập, xử lý và truyền kết quả ra bên ngoài\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mô hình học theo nhóm kết hợp (boosting/ adaboost)\n",
    "- Mô hình học kết hợp nghiện cứu việc kết hợp các hàm fi để tạo ra bộ phân lớp tốt hơn.\n",
    "- Mô hình học mạnh\n",
    "- Mô hình học yếu hơn: có độ chính xác thấp nhưng phải hơn phân lớp ngẫu nhiên\n",
    "- Vấn đề trong mô hình học kết hợp là xác định các trọng số kết họp của các mô hình yếu để tạo thành mô hình mạnh hơn.\n",
    "- Vấn đề chia bộ dữ liệu để đạt được mô hình mạnh hơn.\n",
    "- Adaboost = Adaptive Boosting = tăng cường thích nghi\n",
    "\n",
    "# Slide\n",
    "- Boosting vá Adaboost: học theo nhóm kết hợp:\n",
    "# Ý tưởng:\n",
    "- Kết hợp nhiều quan điểm cá nhân để đưa ra quan điểm cuỗi cùng: ứng dụng thành công trong các lĩnh vực: tài chính, sinh tin học, y khoa, sản xuất, địa lý và truy vẫn ảnh.\n",
    "\n",
    "# Định lý \"Condorcet's Jury\"\n",
    "- Chỉ có 2 giá trị đúng, sai\n",
    "- Có hai giới hạn:\n",
    "  + Giả sử độc lập của các voter (trong thực tế khó đạt được).\n",
    "  + Định lý chỉ áp dụng trên việc bình chọn trên 2 khả năng.\n",
    "\n",
    "# Trí tuệ tập thể\n",
    "- Tính đa dạng của quan điểm: mối thành viên có thông tin của riêng mình thậm chí có các giải thích lập dị.\n",
    "- Tính đọc lập: không ảnh hưởng bỏi các tác động xung quanh.\n",
    "- Tính phân cấp: thành viên có thể rút ra quan điểm dựa trên tri thức cục bộ\n",
    "- Tính tập hợp: tồn tại 1 vài cơ chế cho việc điều chỉnh các phán đoán riêng qua một quyết định tập thể.\n",
    "\n",
    "# Áp dụng học tổ hợp trong máy học\n",
    "- A strong learner\n",
    "- A weak leaner\n",
    "- Tại sao phải học tổ hợp:\n",
    "  + Vì khó tạo trực tiếp được\n",
    "  + Dễ tạo weak learner\n",
    "\n",
    "# Phân tích bias và variance\n",
    "- Intrisic(bản chất): Chủ yếu do tập huấn luyện gây ra\n",
    "- Variance: phương sai\n",
    "- Bias\n",
    "\n",
    "# Kinh nghiệm thực nghiệm\n",
    "- Mô hình đơn giản có bias error cao hơn và variance error nhỏ hơn mô hình phức tạp\n",
    "- Bagging: thu nhỏ variance\n",
    "- Adaboost thu nhỏ cả hai\n",
    "- Bias giảm trong những tác động ban đầu, còn variance giảim trong những tác động sau.\n",
    "\n",
    "# Nguyên tắc Occam's Razor\n",
    "- 2 lớp có cùng lỗi tổng hợp (generation error), 2 phân lớp có cùng lỗi huấn luyện (trainging set error) nên chọn phân lớp đơn giản.\n",
    "\n",
    "# Thuật toán Bagging\n",
    "- Bootstrap Aggregating (boostrap): Có thể hiểu là tập con của tập huấn luyện\n",
    "\n",
    "# AdaBoost:\n",
    "- Đánh trọng số cho từng mẫu huấn luyện\n",
    "- Cập nhật bộ trọng số sau mỗi lần lặp để tập trung (focus) vào các mẫu khó phân lớp, phân lớp sai\n",
    "- Kết hợp phân lớp yếu thành phân lớp mạnh\n",
    "- Tỷ lệ lỗi càng nhỏ thì trọng số càng lớn\n",
    "\n",
    "# Từ vựng\n",
    "- Intrisic: Bản chất"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mô hình thuật giải di truyền (GA: Generic Algorithm)\n",
    "- Là thuật giải tìm kiếm, lựa chọn các giải thuật tối ưu để giải quyết các bài toán thực tế khác nhau dựa trên cơ chế chọn lọc tự nhiên => tối ưu toàn cục.\n",
    "- Xây dụng cấu trúc gen cho mỗi lời giải của bài toán, từ đó mã hóa thành 1 nhiễm sác thể.\n",
    "- Giải mã các nhiễm sắc thể thành lời giải của bài toán. Và đo lượng mức độ lỗi của lời giải.\n",
    "- Xây dựng bài toán theo cơ chế di truyền để tìm kiếm các nhiễm sác thể tốt trên không gian lời giải.\n",
    "\n",
    "# Slide\n",
    "- Giải thuật di truyêng (GA - Genetic Algorithms): Là giải thuật tìm kiếm, chọn lựa các giải pháp tối ưu để giải quyết các bài toán thực tế khác nhau, dựa trên cơ chế chọn lọc tự nhiên: từ tập lời giải ban đầu, thông qua nhiều bước tiến hóa, hình thành tập lời giải mới phù hợp hơn, và cuối cùng dẫn đến lời giải tối ưu toàn cục.\n",
    "- Mỗi Gen mã hóa một đặc điểm, các tính trạng của gen trong bộ gen được gọi là kiểu gen.\n",
    "- Từ kiểu gen, với sự phát triển của sinh vật sẽ hình thành kiểu hình.\n",
    "\n",
    "# Mã hóa\n",
    "- Gồm 2 bước:\n",
    " + Xây dựng cấu trúc gen cho mỗi lời giải của bài toán từ đó mã hóa thành một NST (chuỗi các gen).\n",
    " + Giải mã các NDT để nhận được lời giải.\n",
    "\n",
    "- Tùy thuộc vào nội dung của mỗi bài toán mà ta có cách mã hóa khác nhau:\n",
    "  + Mã hóa dạng chuỗi nhị phân:\n",
    "  + Mã hóa dạng q-phân: được dùng trong trường hợp giá trị của alen không là lũy thừ của 2\n",
    "  + Mã hóa thứ tự\n",
    "  + Mã hóa theo giá trị\n",
    "  + Mã hóa dạng cây\n",
    "\n",
    "# Các yếu tố trong thuật giải di truyền\n",
    "- Quần thể ban đầu:\n",
    "  + Được khỏi tạo từ các cá thể được tạo ngẫu nhiên\n",
    "  + Một tập các lời giải cung cấp bởi các chuyên gia\n",
    "  + Một tập các lời giải cung cấp bởi cá thuật toán tìm kiếm khác\n",
    "- Điều kiện dừng (termination):\n",
    "  + Thiết lập điều kiện dừng:\n",
    "    - Hàm thích nghi (fitness)\n",
    "    - Thời gian chạy\n",
    "    - Sự đa dạng của quần thể (dừng khi sự đa dạng giảm xuống dưới ngưỡng)\n",
    "- Hàm thích nghi (fitness):\n",
    "  + Mục đính của hàm thích nghi là:\n",
    "    - Chọn các cá thể thích nghi cho thế hệ sau.\n",
    "    - Kiểm tra sự hội tụ của giải thuật\n",
    "- Các phép toán di truyền\n",
    "  + Phép chọn lọc (selection): Mục đích của phép lọc là tập trung sự tìm kiếm nghiệm trên miền nghiệm thích nghi nhất\n",
    "  + Phép lai (Crossover): Quá trình kết hợp 2 cá thể cha mẹ đã được lựa chọn để tạo ra các cá thể con\n",
    "  + Phép đột biến (Mutation)\n",
    "\n",
    "# Các phép chọn lọc thường được sử dụng bao gồm:\n",
    "1. Roulette Wheel Selection - Chọn lọc ngẫu nhiên theo vòng tròn Roulette\n",
    "2. Stochastic universal sampling Selection (SUS): Cải tiến của Roulette Wheel Selection: ramdom 1 lần suy ra 3 điểm còn lại.\n",
    "3. Linear Ranking Selection - Chọn lọc theo thứ hạng tuyến tính:\n",
    "  + Ý tưởng: Sắp xếp các cá thể theo thứ tự giảm dần của độ thích nghi\n",
    "  + Từ số ngẫu nhiên r trong khoảng [0, 1], tìm thứ hạng -> tìm được cá thể cha mẹ. Lặp lại m lần (m số cá thể cha mẹ cần lựa chọn)\n",
    "4. Tournament Selection - Chọn lọc theo cạnh tranh:\n",
    "  + Ý tưởng: Lấy ngẫu nhiên k cá thể (k được gọi là tour size) và so sánh độ thích nghi giữa chúng, sau đó chọn ra cá thể có độ thích nghi cao nhất. Quá trình này được lặp lại đúng m lần.\n",
    "\n",
    "# Các phương pháp lai ghép:\n",
    "- Lai 1 điểm, lai m điểm\n",
    "- Lai phi tuyến (non-linear crossover):\n",
    "  + Phép lai phi tuyến tính sẽ tạo ra những nghiệm sai không mong muốn™\n",
    "- Phép lai n điểm tổng quát (Generalized n-point crossover)\n",
    "=> nhận xét:\n",
    "  + lai 1 điểm hay lại m điểm là phép lai tuyến tính chỉ sử dụng cho các bài toán biểu diễn NST bằng chuỗi nhị phân\n",
    "  + Lai 1 điêmt hạn chế việc trao đổi thông tin giữa hai cá thể cha mẹ hơn phép lai m điểm.\n",
    "  + Lai phi tuyến sử dụng cho các bài toán không gian mẫu được biểu diễn bằng các hoán vị.\n",
    "\n",
    "# Các phương pháp đột biến:\n",
    "- đột biến là thay đổi các bit trên nhiễm sắc thể khác nhau để tạo ra tính đa dạng\n",
    "- Toán tử đột biến được xây dựng để tránh việc nhận được giá trị tối ưu cục bộ mỗi gen trên nhiễm sắc thể có tỉ lệ đột biến 1/l (l là chiều dài).\n",
    "- trường hợp không phải là giá trị 0, 1 thì chọn phương pháp hoán vị.\n",
    "- chọn một vài giá trị rồi thêm bớt cộng trừ.\n",
    "\n",
    "# Note:\n",
    "- Vấn đề tối ưu hóa\n",
    "- Bài toán mã hóa: bỏ đồ vào ba lô\n",
    "- Một số cách mã hóa thông tin trong thuật giải di truyền\n",
    "- Hiểu thuật toán lựa chọn\n",
    "\n",
    "# Đặt vấn đề\n",
    "- Thuật giải di truyền truyền truyền thống không phải là thuật toán tối ưu nhất:\n",
    "  + Chọn lọc ngẫu nhiên: Cá thể tốt của quần thể có thể không được chọn lọc để sinh ra trong lần lai tạo kế tiếp.\n",
    "  + Hai cá thể tương tự nhau cho lai tạo có thể không cho ra cá thể tốt.\n",
    "- Nhưng nếu áp dụng chọn lọc nghiêm khác thì quần thể không có tính đa dạng.\n",
    "\n",
    "# Breeder GA: giải thuật người chăn nuôi di truyền.\n",
    "- BGA sử dụng vector các giá trị thực cho phép biểu diễn gần với thực tế hơn những GA thông thường: thực hiện dựa trên chọn lọc nhân tạo tương tự những gì con người thực hiện nhân giống động thực vật.\n",
    "- sử dụng mô hình chọn lọc xén (truncation): T% tỉ lệ xén: những cá thể tốt nhất được chọn lọc và gây giống và gây giống 1 cách ngẫu nhiên cho đến khi số lượng con cháu đạt kích thước của quần thể. Thế hệ con cháu sẽ thay thế thế hệ bố mẹ.\n",
    "- Thuật giải:\n",
    "  1. String length\n",
    "  2. Initial frequency of desirable allele\n",
    "  3. Population size\n",
    "  4. Mutation rate\n",
    "  5. Selection intensity\n",
    "\n",
    "# Cataclysmic Hux Combination: CHC\n",
    "- Lai tạo được nhấn mạnh nhiều\n",
    "- Sử dụng chọn lọc Elitism\n",
    "- Hầu như đột biến không sử dụng\n",
    "- Duy trì tính đa dạng quần thể\n",
    "- Hạn chế phiên bản gây rói (incest): nghĩa là tưởng tự nhau không được ghép nói với nhau.\n",
    "\n",
    "# GAVaPS\n",
    "- Kích thước quần thể là một trong những lựa chọn quan trọng mà tất cả người sử dụng di truyền phải đương đầu:\n",
    "  + Kích thước nhỏ => hội tự nhanh\n",
    "  + Kích thước quá lớn thì có tính đa dạng quần thể nhưng hao phí tài nguyên lớn.\n",
    "- Trong thuật giải di truyền hai vấn đề quan trọng là:\n",
    "  + Duy trì tính đa dạng quần thể (maintenance of diversity)\n",
    "  + Áp dụng chọn lọc (Selection intensity)\n",
    "- Không dùng cơ chế chọn lọc nào mà dùng khái niệm tuổi của 1 nhiễm sắc thể, khái niệm tuổi thay thế cho khái niệm chọn lọc\n",
    "\n",
    "# Từ vựng:\n",
    "- Chromosomes: nhiễm sắc thể\n",
    "- Termination: chấm dứt\n",
    "- Crossover: bắt chéo\n",
    "- Mutation: đột biến\n",
    "- offspring: con cháu\n",
    "- permutation: Hoán vị\n",
    "- designate: chỉ định\n",
    "- desirable: mong muốn\n",
    "- intensity: cường độ\n",
    "- incest: loạn luân\n",
    "- diverge: Phân ra\n",
    "- diversity: Phân loại\n",
    "- preproduction: sinh sản\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
