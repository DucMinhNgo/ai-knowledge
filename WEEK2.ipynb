{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbada3ec",
   "metadata": {},
   "source": [
    "# Reinformance learning:\n",
    "- Agent: thực hiện hành động (action) làm thay đổi môi trường. Chọn hành động để mang lại reward cao nhất\n",
    "  + Đạt điểm thưởng cao trong 1 chuỗi hành động.\n",
    "- Environment: trạng thái mới (state), điểm thưởng (reward), Fully/Partially-observable environment.\n",
    "  + Ví dụ: Chơi cờ, chơi pocker.\n",
    "- Rewards: positive, negative, largest reward, thường biểu diễn dưới dạng vector số.\n",
    "- Actuator:\n",
    "- Sensors:\n",
    "- State: được biểu diễn dưới dạng vector, ma trận:\n",
    "- Observation:\n",
    "- Action space: bao gồm tất cả hành động của môi trường:\n",
    "  + Discrete action space:\n",
    "  + Continuous action space:\n",
    "- Policy (hàm): một policy tốt khi agent đưa ra hành động tốt.\n",
    "  + stimulus-response: kích hoạt, phản hồi\n",
    "  + acssociations: liên kết.\n",
    "  + 2 loại:\n",
    "    - Categorical policy:\n",
    "    - gaussian policy: Agent lái xe: chọn tốc độ lái xe cho thuật toán Gauss:\n",
    "  + reward signal: tín hiệu điểm thưởng\n",
    "  + Điều chỉnh chính sách liên tục:\n",
    "- Expected return:\n",
    "- Modal:\n",
    "  - (free) Try and error.\n",
    "  - \n",
    "\n",
    "Xây dựng policy như thế nào?\n",
    "Agent, Reward, Policy\n",
    "\n",
    "- Exploration: thăm dò (ưu tiên hàng động đã thử trong quá khứ).\n",
    "- Exploitation: khai thác (Thử những hành động chưa được chọn trước đó): rủi ro gặp hành động xấu:\n",
    "- Multiple-armed bandit problem:\n",
    "- Giải thuật:\n",
    "  + greedy algorithm:\n",
    "  + binary indicator:\n",
    "- Upper confident bound:\n",
    "  + Ban đầu xa điểm đích: cho phép thăm dò nhiều, càng về đích thì hạn chế thăm dò.\n",
    "- Hoeffding: dùng khi tập chứ liệu có chặn.\n",
    "- 4 pp:\n",
    "  + e-Greedy\n",
    "  + UCB1\n",
    "  + Bayesian UCB\n",
    "  + Thomson Sampling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7f5848d",
   "metadata": {},
   "source": [
    "# Bài toán học tăng cường (lập kế hoạch).\n",
    "- Reinforcement learning:\n",
    " + Bài toán tìm các đưa ra 1 chuỗi hành động, quyết định để đạt mục tiêu. Đưa ra nhiều quyết định.\n",
    " + Khi có kế hoạch có thể đánh giá kế hoạch tốt hay xấu nhưng đưa ra kế hoạch thì khó và không biết cách đưa ra kế hoạch.\n",
    "\n",
    "- Học tăng cường:\n",
    "  + Là lập kế hoạch trong tình huống khi có một cơ chế đánh giá từng hành động nhưng cơ chế đánh giá chưa biết trước nhưng được phép thử. Bản thân không biết trước số điểm, đánh giá đó.\n",
    "  + Nhiệm vụ: lập kế hoạch để lấy được tối đa tổng điểm thưởng.\n",
    "\n",
    "Quá trình quyết định Markov: Mô mình mà trạng thái của bước sau luôn phụ thuộc vào trạng thái của bước trước.\n",
    "# Các khái niệm đố với quá trình đưa ra quyết định Markov:\n",
    "# Chiến lược (policy)\n",
    "Cách chọn hành động A trên mỗi trạng thái S (cách đưa ra quyết định trong mọi trạng thái có thể)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "109ad764",
   "metadata": {},
   "source": [
    "# Phương trình học tăng cường với ứng dụng deep Qlearning chơi game CartPole:\n",
    "Bản chất của RL: trial and error nghĩa là thử đi thử lại và rút ra kinh nghiệm sau mỗi lần thử như vậy. DeepMind (AlphaGo, AlphaZero, AlphaStar,...).\n",
    "Bao gồm 7 khái niệm.\n",
    "1) Agent\n",
    "2) Everonment\n",
    "3) State\n",
    "4) Action\n",
    "5) Reward\n",
    "6) Episode (tập phim): Một loạt các tưởng tác của agent từ điểm đầu đến môi trưởng được gọi là Episode\n",
    "7) Policy\n",
    "\n",
    "# Markov Decision Process (MDP):\n",
    "MDP là một framework giúp agent đưa ra quyết đihj tại 1 state nào đó. mỗi state chỉ phụ thuộc vào state trước đó và xác suất chuyển đổi giữa hai state: markov property là tính không nhớ (memoryless). Mọi sự việc đều là ngẫu nhiên (stochatic process).\n",
    "Có thể áp dụng MDP để biểu diễn bài toán dưới dạng\n",
    "(S, A, R, P)\n",
    "P: phân bố xác suất chuyến đổ.\n",
    "\n",
    "# Q-learning:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 10:44:50) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
